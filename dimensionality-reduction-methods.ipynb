{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dimensionality Reduction Methods","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n* [PCA](#PCA)\n    * [Variance and Covariance](#Variance)\n    * [Method](#Method_PCA)\n    * [Coding Example](#Codingpca)\n* [Kernel PCA](#Kernel_PCA)\n    * [](#section_2_1)\n    * [Method](#Method_Kernel)\n    * [Coding Example](#Coding_Kernel)\n    * [Section 2.2](#section_2_2)\n* [LDA](#LDA)\n    * [Section 3.1](#section_3_1)\n    * [Sub Section 3.1.1](#sub_section_3_1_1)\n    * [Sub Section 3.1.2](#sub_section_3_1_2)\n    * [Section 3.2](#section_3_2)\n* [t-SNE](#t-SNE)","metadata":{}},{"cell_type":"markdown","source":"# 1. Principal Component Analysis (PCA)<a class=\"anchor\" id=\"PCA\"></a>\nPrincipal Component Analysis (PCA) is a method that takes care of the *curse of dimensionality*, this means that sometimes too many features (dimensions) can affect negativeley the impact of how our model will perform. What PCA does, is it takes the data in the feature space $S$ of dimension $n=\\text{number of features}$, then in some way chooses a new space $S_*$ with a dimension $k$ lower than the number of features $n$ and projects our data into this new space $S_*$. The basis vectors of this new space $S_*$ are called the *principal components*. How do we find the principal components? We must first introduce variance and covariance.","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Variance and Covariance <a class=\"anchor\" id=\"Variance\"></a>\nVariance measures the variation of a single random variable, given by the formlula:\n$$Var(x) = \\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})^2.$$\nThe covariance measures the direction of the relationship between two variables, given by:\n$$Cov(x,y) = \\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y}).$$\n\nThis allows us to introduce the covariance matrix:\n$$M = \\begin{pmatrix}\nVar(x) & Cov(x,y) \\\\\nCov(y,x) & Var(y)\n\\end{pmatrix}.$$\n\nLet's say our data is represented by a matrix $X$, whose columns are vectors $\\bar{x}_i$. One can check that the above definitions lead to the following matrix formula for the covariance matrix:\n$$M= \\frac{1}{n-1}\\sum_{i=1}^n (\\overrightarrow{X}_i - \\bar{X})(\\overrightarrow{X}_i - \\bar{X})^{T},$$\nwhere $\\bar{X}$ is the vector showing the mean of the data points.\n\n**In an ideal dataset we would want maximal variance and minimal covariance. We want maximal variance because, high variance on a feature $x$ means that feature differentiates well each sample of the dataset. We want minimal covariance because, low covariance of features $x$ and $y$ means that these two features are not correlated. How do we minimize these values?**\n\nFirst let's see how to get, mathematically, eigenvalues and eigenvectors of the square matrix $M$ (There's also python functions to calculate them). \n\nSuppose we have an eigenvector $v_1$ of $M$, that is $Mv_1=\\lambda_1 v_1$ for some eigenvalue $\\lambda_1 \\in \\mathbb{R}$. To determine the eigenvalues we solve for $\\text{det}(M-\\lambda I)=0$, the eigenvectors then can be computed by solving the system of simultaneous equations given by $M \\overrightarrow{x}-\\lambda_i \\overrightarrow{x}=(M-\\lambda_i I)\\overrightarrow{x}=0$. \n\nNow that if we have our eigenvectors $v_1, v_2$ and eigenvalues $\\lambda_1,\\lambda_2$, we can change the basis of the space $S$, and set it equal to the eigenvectors. This means that in a our new basis, our matrix will now be expressed in the form:\n$$M = \\begin{pmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{pmatrix}.$$\nWe get a diagonal matrix because $M$ is symmetric. Let's give a proof:\n\nBy the *Spectral theorem* [METER], $M$ is diagonalizable if there exists a diagonal matrix $D$ and an invertible matrix $A$ such that $$M = ADA^{-1}.$$\n\nWe build two new matrices:\n$$V = (v_1, v_2)\\\\\nE = \\begin{pmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{pmatrix}.$$\nTherefore by definition we have:\n$$MV=VE,$$\nwhich is the same as:\n\\begin{align*}M &=VEV^{-1}.\\\\\n\\end{align*}\nMeaning we can diagonalize $M$ as we did.\n","metadata":{}},{"cell_type":"markdown","source":"## 1.2. Method <a class=\"anchor\" id=\"Method_PCA\"></a>\nWe have shown how to find linear combinations of the features and get features that have high variance, however our dimension is still the same. We can reduce the dimension to k by choosing the k principal components (eigenvectors) which have the highest variance (highest eigenvalues).\nHere is an overvoew of the method:\n\n1. Standardize the data: Before performing PCA, it is important to standardize the data so that each feature has zero mean and unit variance. This is done to ensure that features with larger scales do not dominate the analysis.\n\n2. Compute the covariance matrix: PCA computes the covariance matrix of the standardized data. The covariance matrix captures the relationships between the features in the data.\n\n3. Compute the eigenvectors and eigenvalues: PCA then computes the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in the data that contain the most variance, and the eigenvalues represent the amount of variance explained by each eigenvector.\n\n4. Select the principal components: PCA selects the top k eigenvectors that explain the most variance in the data. These eigenvectors are called the principal components. The number of principal components selected corresponds to the desired number of dimensions in the reduced data.\n\n5. Project the data onto the principal components: Finally, PCA projects the standardized data onto the principal components to obtain the reduced data. This is done by multiplying the standardized data matrix by the matrix of the top k eigenvectors.\n\nThe resulting reduced data has fewer dimensions than the original data, but still retains most of the variation in the data. This reduction in dimensions makes it easier to visualize and analyze the data, while still capturing the most important features of the data.\n","metadata":{}},{"cell_type":"markdown","source":"## 1.3 Coding Example <a class=\"anchor\" id=\"Codingpca\"></a>\n### 1.3a. Loading data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3b. Scaling the data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3c. Computing covariance matrix","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3d. Find eigenvalues and eigenvectors","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select principal components","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Project data on principal components","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Kernel PCA\n\nPCA works well when linear transformations work to find the best principal components. However if data is not linearly distributed a linear transformation might not be enough. The idea is to visualize our original data in a higher dimensional space where now the data is linearly distributed. Now we could perform PCA, however this may be computationally expensive since our new feature space may be too high dimensional. This is where the *kernel trick* comes in.","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Kernel Trick\n\nSuppose we have a non-linear transformation $\\phi(x)$ from our original feature space $S$ of dimension $n$ to our new feature space $S^{*}$ of dimension $m>n$. After standarizing the data (it follows $\\overline{\\phi(X)}=0$), we have that the covariance matrix in $S^{*}$ becomes:\n$$M= \\frac{1}{n-1}\\sum_{i=1}^n \\phi({\\overrightarrow{X}_i} )\\phi(\\overrightarrow{X}_i )^{T}.$$\n\nLet $v_k$ and $\\lambda_k$ be eigenvectors and eigenvalues respectively, we have \n\\begin{align*}\nv_k &= \\frac{M v_k}{\\lambda_k}\\\\\n&= \\frac{1}{\\lambda_k(n-1)}\\sum_{j=1}^n \\phi({\\overrightarrow{X}_j} )\\phi(\\overrightarrow{X}_j )^{T})v_k\\\\\n&= \\frac{1}{\\lambda_k(n-1)}\\sum_{j=1}^n \\phi({\\overrightarrow{X}_j} )(\\phi(\\overrightarrow{X}_j )^{T}v_k)\\\\\n&= \\sum_{j=1}^n \\frac{\\phi(\\overrightarrow{X}_j)^{T}v_k}{\\lambda_k(n-1)} \\phi({\\overrightarrow{X}_j} )\\\\\n&= \\sum_{j=1}^n a_{kj} \\phi({\\overrightarrow{X}_j} ).\n\\end{align*}\n\nThus, substituting the above into $\\lambda_k v_k = M v_k$ we get\n\n$$\\lambda_k \\sum_{j=1}^n a_{kj} \\phi({\\overrightarrow{X}_j} ) = \\frac{1}{n-1}\\sum_{i=1}^n \\phi({\\overrightarrow{X}_i} )\\phi(\\overrightarrow{X}_i )^{T} (\\sum_{j=1}^n a_{kj} \\phi({\\overrightarrow{X}_j})),$$\nwe would want to substitute above the kernel $\\kappa(\\overrightarrow{X_i}, \\overrightarrow{X_j})= \\phi(\\overrightarrow{X_j})\\phi(\\overrightarrow{X_i})^{T}$, to do this we just multiply both sides by $\\phi(\\overrightarrow{X_l})^T$, obtaining\n\n$$\\lambda_k \\sum_{j=1}^n a_{kj}\\kappa(\\overrightarrow{X_l}, \\overrightarrow{X_j}) = \\frac{1}{n-1}\\sum_{i=1}^n \\kappa(\\overrightarrow{X_l}, \\overrightarrow{X_i})(\\sum_{j=1}^n a_{kj} \\kappa(\\overrightarrow{X_i}, \\overrightarrow{X_j})).$$\n\nThis can be expressed in matrix notation as \n$$\\lambda_k A_k K = \\frac{1}{n-1}A_k K^2,$$\nwhere $K_{i,j}= \\kappa(\\overrightarrow{X_i}, \\overrightarrow{X_j})$ and $A_k = [a_{k1}, \\cdots , a_{kn}]^T$. Which can be simplified to\n$$\\lambda_k A_k= \\frac{1}{n-1}A_kK.$$\nMeaning that $\\lambda_k (n-1)$ is the corresponding eigenvalue of the eigenvector $A_k$ of the matrix $K$.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Method \nklk","metadata":{}},{"cell_type":"markdown","source":"# 3. Linear Discriminant Analysis (LDA)\n\nLDA is a linear transformation method of the data, similar to PCA. However it is a supervised algorithm, hence it uses the information we have about the class labels. Similarly to PCA, we will get a lower dimensional space whose basis are going to be the eigenvectors of some matrix. The difference is that this matrix is different, we are going to use a matrix that contains information of how class labels are distributed in data. This is the","metadata":{}},{"cell_type":"markdown","source":"## Summary\n1. Standardize the data: Before performing LDA, it is important to standardize the data so that each feature has zero mean and unit variance. This is done to ensure that features with larger scales do not dominate the analysis.\n\n2. Compute the class means: LDA computes the mean of each feature for each class in the data.\n\n3. Compute the within-class scatter matrix: LDA then computes the within-class scatter matrix, which measures the variation of the data within each class. This is done by calculating the covariance matrix of each class and summing them up.\n\n4. Compute the between-class scatter matrix: LDA also computes the between-class scatter matrix, which measures the variation between the classes. This is done by calculating the mean difference between the classes and summing them up.\n\n5. Compute the eigenvectors and eigenvalues: LDA then computes the eigenvectors and eigenvalues of the matrix $S_w^{-1} S_b$, where $S_w$ is the within-class scatter matrix and $S_b$ is the between-class scatter matrix. The eigenvectors represent the directions in the data that separate the classes the most, and the eigenvalues represent the amount of separation captured by each eigenvector.\n\n6. Select the discriminant functions: LDA selects the top k eigenvectors that separate the classes the most. These eigenvectors are called the discriminant functions. The number of discriminant functions selected corresponds to the desired number of dimensions in the reduced data.\n\n7. Project the data onto the discriminant functions: Finally, LDA projects the standardized data onto the discriminant functions to obtain the reduced data. This is done by multiplying the standardized data matrix by the matrix of the top k eigenvectors.\n\nThe resulting reduced data has fewer dimensions than the original data, but still retains most of the information needed for classification. This reduction in dimensions makes it easier to visualize and analyze the data, while still capturing the most important features for classification.\n\nTo classify new data points using LDA, we project the new data onto the discriminant functions and assign it to the class with the highest probability based on the training data.","metadata":{}},{"cell_type":"markdown","source":"# 4. t-Distributed Stochastic Neighbour Embedding (t-SNE)","metadata":{}},{"cell_type":"markdown","source":"## Summary\n\n1. Compute pairwise similarities: t-SNE first computes the pairwise similarities between all data points in high-dimensional space. Similarity is defined as a probability distribution that measures the similarity between two data points.\n\n2. Compute similarity in low-dimensional space: t-SNE then defines a similar pairwise similarity between the same data points in a low-dimensional space. The low-dimensional similarities are defined as a Student-t distribution, which has heavier tails than the normal distribution, allowing for better separation of nearby points.\n\n3. Optimize the embeddings: t-SNE then finds the optimal low-dimensional embeddings that best match the pairwise similarities computed in step 1 and step 2. This is done by minimizing the KL divergence between the two similarity distributions while preserving the distances between the data points.\n\n4. Iterate the optimization: t-SNE performs multiple iterations of the optimization process, gradually improving the embedding with each iteration until convergence is reached.\n\nThe resulting low-dimensional embeddings can be visualized in a 2D or 3D plot, allowing for easy visualization and interpretation of high-dimensional data. t-SNE is particularly useful for exploring complex datasets with many features, such as images, text, and genetic data. However, it should be noted that t-SNE is a nonlinear algorithm and can be sensitive to its hyperparameters, so it is important to choose them carefully for best results.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}