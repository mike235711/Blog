{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f39955f",
   "metadata": {},
   "source": [
    "A $discrete$ classification model gives an output of either $0$ or $1$. \n",
    "\n",
    "A $continuous$ classification model calculates the probability or score of each individual being a positive (suvived) or negative (died), then it uses a threshold to determine if it actually classifies this individual as positive or negative. \n",
    "\n",
    "This is a binary classification problem (we want to tell if a person suvived 1 or not 0). To my knowledge there are 4 binary classification metrics:\n",
    "<ol>\n",
    "    <li>Accuracy - Measures how many observations, both positive and negative, were correctly classified.</li>\n",
    "     $$Acc= \\frac{tp+tn}{fp+fn+tp+tn}.$$\n",
    "    This metric measures how many observations, both positive and negative were correctly classified. \n",
    "   \n",
    "**This works well for balanced problems. However when there is an inbalance (i.e. number of positives >>number of negatives), if our model just randomly sets almost all to positives we are going to get an incorrect high accuracy.**\n",
    "    <li>F1 score (or Fbeta score)</li>\n",
    "    $$F_{\\beta}= (1+\\beta^2)\\frac{precision*recall}{\\beta^2*precision+recall},$$\n",
    "    where :\n",
    "    <ul>\n",
    "     <li>$precision= \\frac{tp}{tp+fp}$ (measures how many of the ones classified as positives were actually positives, i.e. measures the probablity that if a model gives positive, it is actually positive),</li> \n",
    "     <li>$recall = \\frac{tp}{tp+fn}$ (measures how many of the ones that should be classified as positives were classified as positives, i.e. measures the probability that the model will identify a known positive as positive). </li>\n",
    "    </ul>\n",
    "    \n",
    "Therefore this metric gives a $harmonic\\; mean$ between precision and recall. Basically high precision and recall gives high F1 score. The $\\beta$ is used to control to which of precision or recall we give more importance (1 gives both equal importance, lower gives precision more importance).\n",
    "\n",
    "**This works well when we care more about the positive class, i.e. recognizing a heart disease (positive) or not (negative). This is mainly because precision and recall are only using positive info.**\n",
    "    <li>ROC AUC (Reciever Opening Characteristic Area Under Curve)</li>\n",
    "ROC is a plot that visualizes for every threshold, the tradeoff between true positive rate (TPR) and false positive rate (FPR).\n",
    "    \n",
    "<img src=\"attachment:5aa409d8-8d97-4527-b2d1-b3f8e136f3c6.png\" width=\"200px\" height=\"200px\">\n",
    "    \n",
    "Each curve represents a model, where the curve shows how TPR and FPR at each threshold. The random model is a line $y=x$ because it will be (on average) predicting the same TPR as FPR.\n",
    "\n",
    "However it is sometimes difficult to observe for which model the ROC curve gives best results, to get this info we find the area under the curve. (since a perfect model would have an area of 1). \n",
    "    \n",
    "**This method should be used if we care only about ranking predictions and not about giving the probabilities.\n",
    " However there has been research [Takaya, Marc], indicating that if the class is skewed (i.e. positives>>negatives) this is not the best method.**\n",
    "    <li>PR AUC</li>\n",
    "This is similar to ROC AUC, but instead we use a Precision-Recall curve. I think of it as an alternative to ROC AUC **when the class is heavily skewed. This is also great when we want to decide which threshold to use.**\n",
    "    \n",
    "<img src=\"attachment:14b1db54-4b95-47f8-bfe3-e7db2829a6b9.png\" width=\"300px\" height=\"300px\">\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab52d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5d22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
